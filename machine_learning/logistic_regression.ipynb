{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Logistic Regression 로지스틱 회귀 0 ~ 1 이진분류\n",
                "# https://github.com/rickiepark/hg-mldl/blob/master/4-1.ipynb\n",
                "# SGD (Stochastic Gradient Descent) SGD는 경사 하강법의 변형으로, 머신러닝 모델의 파라미터를 최적화하기 위해 사용됩니다. 주로 대규모 데이터셋이나 온라인 학습(배치가 아닌 실시간 데이터 학습)에 적합합니다.\n",
                "# SVM (Support Vector Machine) 초평면: 데이터 클래스를 구분하는 결정 경계입니다 서포트 벡터: 결정 경계를 정의하는데 가장 중요한 데이터 포인트들입니다"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "<class 'numpy.ndarray'>\n",
                        "One-vs-Rest 방식의 예측: 0.85 ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch' 'Bream' 'Smelt' 'Perch' 'Perch'\n",
                        " 'Pike' 'Bream' 'Perch' 'Bream' 'Parkki' 'Bream' 'Bream' 'Perch' 'Perch'\n",
                        " 'Perch' 'Bream' 'Smelt' 'Bream' 'Bream' 'Bream' 'Bream' 'Perch' 'Perch'\n",
                        " 'Perch' 'Smelt' 'Smelt' 'Pike' 'Perch' 'Perch' 'Pike' 'Bream' 'Perch'\n",
                        " 'Perch' 'Perch' 'Parkki' 'Perch']\n",
                        "Multinomial 방식의 예측: 0.85 ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch' 'Bream' 'Smelt' 'Perch' 'Perch'\n",
                        " 'Pike' 'Bream' 'Perch' 'Bream' 'Parkki' 'Bream' 'Bream' 'Perch' 'Perch'\n",
                        " 'Perch' 'Bream' 'Smelt' 'Bream' 'Bream' 'Bream' 'Bream' 'Perch' 'Perch'\n",
                        " 'Perch' 'Smelt' 'Smelt' 'Pike' 'Perch' 'Perch' 'Pike' 'Bream' 'Perch'\n",
                        " 'Perch' 'Perch' 'Parkki' 'Perch']\n"
                    ]
                }
            ],
            "source": [
                "fish = pd.read_csv('http://bit.ly/fish_csv_data')\n",
                "fish['Species'].unique() # ['Bream', 'Roach', 'Whitefish', 'Parkki', 'Perch', 'Pike', 'Smelt']\n",
                "fish.head(2)\n",
                "fish.columns # ['Species', 'Weight', 'Length', 'Diagonal', 'Height', 'Width']\n",
                "# fish = fish.loc[(fish['Species']=='Bream') | (fish['Species']=='Smelt')] #Bream, Smelt 두개만 , 이진분류\n",
                "fish1 = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']]\n",
                "print(type(fish1))# <class 'pandas.core.frame.DataFrame'>\n",
                "fish2 = fish1.values\n",
                "print(type(fish2)) # <class 'numpy.ndarray'>\n",
                "fish3 = fish['Species'].values\n",
                "fish['Species'].unique()\n",
                "X_train, X_test, y_train, y_test = train_test_split(fish2, fish3, random_state=42)\n",
                "# StandardScaler 객체 생성\n",
                "scaler = StandardScaler()\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_test = scaler.fit_transform(X_test)\n",
                "lr = LogisticRegression()\n",
                "lr.fit(X_train, y_train)\n",
                "# print(lr.predict(X_test))\n",
                "# print(lr.predict_proba(X_test)) # 각클래스에 속할 확률\n",
                "# y_test\n",
                "# lr.decision_function(X_test) # 결정함수, 분류할때 얼마나 강하게 결정하였는가\n",
                "\n",
                "lr_ovr = LogisticRegression(max_iter=200)\n",
                "lr_ovr.fit(X_train, y_train)\n",
                "\n",
                "# 다중 클래스 로지스틱 회귀 모델 학습 (Multinomial 방식)\n",
                "lr_multinomial = LogisticRegression(multi_class='multinomial', max_iter=200, solver='lbfgs') # 소프트맥스 회귀 multinomial\n",
                "lr_multinomial.fit(X_train, y_train)\n",
                "\n",
                "# 예측\n",
                "y_pred_ovr = lr_ovr.predict(X_test)\n",
                "y_pred_multinomial = lr_multinomial.predict(X_test)\n",
                "\n",
                "print(\"One-vs-Rest 방식의 예측:\",accuracy_score(y_test, y_pred_ovr), y_pred_ovr)\n",
                "print(\"Multinomial 방식의 예측:\",accuracy_score(y_test, y_pred_multinomial), y_pred_multinomial)\n",
                "# print(\"SGDClassifier Accuracy:\", accuracy_score(y_test, y_pred_sgd))"
            ]
        },
        {
            "attachments": {
                "image.png": {
                    "image/png": "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"
                }
            },
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![image.png](attachment:image.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "<class 'numpy.ndarray'>\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "array(['Bream', 'Smelt'], dtype=object)"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "fish = pd.read_csv('http://bit.ly/fish_csv_data')\n",
                "\n",
                "# fish.info() # object 는 숫자로 바꿔야 한다 머신이에서는 안해도 되네\n",
                "fish['Species'].unique() # ['Bream', 'Roach', 'Whitefish', 'Parkki', 'Perch', 'Pike', 'Smelt']\n",
                "fish.head(2)\n",
                "fish.columns # ['Species', 'Weight', 'Length', 'Diagonal', 'Height', 'Width']\n",
                "fish = fish.loc[(fish['Species']=='Bream') | (fish['Species']=='Smelt')]\n",
                "fish1 = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']]\n",
                "print(type(fish1))# <class 'pandas.core.frame.DataFrame'>\n",
                "fish2 = fish1.values\n",
                "print(type(fish2)) # <class 'numpy.ndarray'>\n",
                "fish3 = fish['Species'].values\n",
                "fish['Species'].unique()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Bream' 'Smelt' 'Smelt' 'Smelt' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream'\n",
                        " 'Bream' 'Bream' 'Bream' 'Bream']\n",
                        "[[0.94907744 0.05092256]\n",
                        " [0.01069346 0.98930654]\n",
                        " [0.02188471 0.97811529]\n",
                        " [0.01047612 0.98952388]\n",
                        " [0.9811643  0.0188357 ]\n",
                        " [0.9932275  0.0067725 ]\n",
                        " [0.99286422 0.00713578]\n",
                        " [0.99296063 0.00703937]\n",
                        " [0.99807479 0.00192521]\n",
                        " [0.98357374 0.01642626]\n",
                        " [0.94029775 0.05970225]\n",
                        " [0.91674258 0.08325742]\n",
                        " [0.99863959 0.00136041]]\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "array([-2.92518437,  4.5273717 ,  3.79983925,  4.54812585, -3.95298567,\n",
                            "       -4.98808948, -4.93547193, -4.94917262, -6.25079442, -4.09231148,\n",
                            "       -2.75682692, -2.39888941, -6.59860535])"
                        ]
                    },
                    "execution_count": 33,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(fish2, fish3, random_state=42)\n",
                "# StandardScaler 객체 생성\n",
                "scaler = StandardScaler()\n",
                "# 모델의 성능 향상: 많은 머신러닝 알고리즘, 특히 경사 하강법을 사용하는 알고리즘(예: 선형 회귀, 로지스틱 회귀, 신경망 등)은 데이터의 스케일에 민감합니다. \n",
                "# 피처의 스케일이 다르면 최적의 솔루션으로 수렴하는 속도가 느려지거나, 제대로 수렴하지 않을 수 있습니다.\n",
                "# 정규화 필요성: KNN, SVM, K-평균과 같은 거리 기반 알고리즘에서는 피처들의 스케일이 다르면 특정 피처가 다른 피처보다 더 큰 영향을 미칠 수 있습니다.\n",
                "\n",
                "# 데이터 표준화 (fit_transform 메서드 사용)\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_test = scaler.fit_transform(X_test)\n",
                "# y_train = scaler.fit_transform(y_train)\n",
                "# y_test = scaler.fit_transform(y_test)\n",
                "\n",
                "# print(X_test)\n",
                "lr = LogisticRegression()\n",
                "lr.fit(X_train, y_train)\n",
                "print(lr.predict(X_test))\n",
                "print(lr.predict_proba(X_test)) # 각클래스에 속할 확률\n",
                "y_test\n",
                "lr.decision_function(X_test) # 결정함수, 분류할때 얼마나 강하게 결정하였는가\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Help on class StandardScaler in module sklearn.preprocessing._data:\n",
                        "\n",
                        "class StandardScaler(sklearn.base.OneToOneFeatureMixin, sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
                        " |  StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
                        " |  \n",
                        " |  Standardize features by removing the mean and scaling to unit variance.\n",
                        " |  \n",
                        " |  The standard score of a sample `x` is calculated as:\n",
                        " |  \n",
                        " |      z = (x - u) / s\n",
                        " |  \n",
                        " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
                        " |  and `s` is the standard deviation of the training samples or one if\n",
                        " |  `with_std=False`.\n",
                        " |  \n",
                        " |  Centering and scaling happen independently on each feature by computing\n",
                        " |  the relevant statistics on the samples in the training set. Mean and\n",
                        " |  standard deviation are then stored to be used on later data using\n",
                        " |  :meth:`transform`.\n",
                        " |  \n",
                        " |  Standardization of a dataset is a common requirement for many\n",
                        " |  machine learning estimators: they might behave badly if the\n",
                        " |  individual features do not more or less look like standard normally\n",
                        " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
                        " |  \n",
                        " |  For instance many elements used in the objective function of\n",
                        " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
                        " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
                        " |  all features are centered around 0 and have variance in the same\n",
                        " |  order. If a feature has a variance that is orders of magnitude larger\n",
                        " |  than others, it might dominate the objective function and make the\n",
                        " |  estimator unable to learn from other features correctly as expected.\n",
                        " |  \n",
                        " |  `StandardScaler` is sensitive to outliers, and the features may scale\n",
                        " |  differently from each other in the presence of outliers. For an example\n",
                        " |  visualization, refer to :ref:`Compare StandardScaler with other scalers\n",
                        " |  <plot_all_scaling_standard_scaler_section>`.\n",
                        " |  \n",
                        " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
                        " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
                        " |  \n",
                        " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
                        " |  \n",
                        " |  Parameters\n",
                        " |  ----------\n",
                        " |  copy : bool, default=True\n",
                        " |      If False, try to avoid a copy and do inplace scaling instead.\n",
                        " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
                        " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
                        " |      returned.\n",
                        " |  \n",
                        " |  with_mean : bool, default=True\n",
                        " |      If True, center the data before scaling.\n",
                        " |      This does not work (and will raise an exception) when attempted on\n",
                        " |      sparse matrices, because centering them entails building a dense\n",
                        " |      matrix which in common use cases is likely to be too large to fit in\n",
                        " |      memory.\n",
                        " |  \n",
                        " |  with_std : bool, default=True\n",
                        " |      If True, scale the data to unit variance (or equivalently,\n",
                        " |      unit standard deviation).\n",
                        " |  \n",
                        " |  Attributes\n",
                        " |  ----------\n",
                        " |  scale_ : ndarray of shape (n_features,) or None\n",
                        " |      Per feature relative scaling of the data to achieve zero mean and unit\n",
                        " |      variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
                        " |      variance is zero, we can't achieve unit variance, and the data is left\n",
                        " |      as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
                        " |      when `with_std=False`.\n",
                        " |  \n",
                        " |      .. versionadded:: 0.17\n",
                        " |         *scale_*\n",
                        " |  \n",
                        " |  mean_ : ndarray of shape (n_features,) or None\n",
                        " |      The mean value for each feature in the training set.\n",
                        " |      Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n",
                        " |  \n",
                        " |  var_ : ndarray of shape (n_features,) or None\n",
                        " |      The variance for each feature in the training set. Used to compute\n",
                        " |      `scale_`. Equal to ``None`` when ``with_mean=False`` and\n",
                        " |      ``with_std=False``.\n",
                        " |  \n",
                        " |  n_features_in_ : int\n",
                        " |      Number of features seen during :term:`fit`.\n",
                        " |  \n",
                        " |      .. versionadded:: 0.24\n",
                        " |  \n",
                        " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
                        " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
                        " |      has feature names that are all strings.\n",
                        " |  \n",
                        " |      .. versionadded:: 1.0\n",
                        " |  \n",
                        " |  n_samples_seen_ : int or ndarray of shape (n_features,)\n",
                        " |      The number of samples processed by the estimator for each feature.\n",
                        " |      If there are no missing samples, the ``n_samples_seen`` will be an\n",
                        " |      integer, otherwise it will be an array of dtype int. If\n",
                        " |      `sample_weights` are used it will be a float (if no missing data)\n",
                        " |      or an array of dtype float that sums the weights seen so far.\n",
                        " |      Will be reset on new calls to fit, but increments across\n",
                        " |      ``partial_fit`` calls.\n",
                        " |  \n",
                        " |  See Also\n",
                        " |  --------\n",
                        " |  scale : Equivalent function without the estimator API.\n",
                        " |  \n",
                        " |  :class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
                        " |      correlation across features with 'whiten=True'.\n",
                        " |  \n",
                        " |  Notes\n",
                        " |  -----\n",
                        " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
                        " |  transform.\n",
                        " |  \n",
                        " |  We use a biased estimator for the standard deviation, equivalent to\n",
                        " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
                        " |  affect model performance.\n",
                        " |  \n",
                        " |  Examples\n",
                        " |  --------\n",
                        " |  >>> from sklearn.preprocessing import StandardScaler\n",
                        " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
                        " |  >>> scaler = StandardScaler()\n",
                        " |  >>> print(scaler.fit(data))\n",
                        " |  StandardScaler()\n",
                        " |  >>> print(scaler.mean_)\n",
                        " |  [0.5 0.5]\n",
                        " |  >>> print(scaler.transform(data))\n",
                        " |  [[-1. -1.]\n",
                        " |   [-1. -1.]\n",
                        " |   [ 1.  1.]\n",
                        " |   [ 1.  1.]]\n",
                        " |  >>> print(scaler.transform([[2, 2]]))\n",
                        " |  [[3. 3.]]\n",
                        " |  \n",
                        " |  Method resolution order:\n",
                        " |      StandardScaler\n",
                        " |      sklearn.base.OneToOneFeatureMixin\n",
                        " |      sklearn.base.TransformerMixin\n",
                        " |      sklearn.utils._set_output._SetOutputMixin\n",
                        " |      sklearn.base.BaseEstimator\n",
                        " |      sklearn.utils._metadata_requests._MetadataRequester\n",
                        " |      builtins.object\n",
                        " |  \n",
                        " |  Methods defined here:\n",
                        " |  \n",
                        " |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
                        " |      Initialize self.  See help(type(self)) for accurate signature.\n",
                        " |  \n",
                        " |  fit(self, X, y=None, sample_weight=None)\n",
                        " |      Compute the mean and std to be used for later scaling.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                        " |          The data used to compute the mean and standard deviation\n",
                        " |          used for later scaling along the features axis.\n",
                        " |      \n",
                        " |      y : None\n",
                        " |          Ignored.\n",
                        " |      \n",
                        " |      sample_weight : array-like of shape (n_samples,), default=None\n",
                        " |          Individual weights for each sample.\n",
                        " |      \n",
                        " |          .. versionadded:: 0.24\n",
                        " |             parameter *sample_weight* support to StandardScaler.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      self : object\n",
                        " |          Fitted scaler.\n",
                        " |  \n",
                        " |  inverse_transform(self, X, copy=None)\n",
                        " |      Scale back the data to the original representation.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                        " |          The data used to scale along the features axis.\n",
                        " |      copy : bool, default=None\n",
                        " |          Copy the input X or not.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
                        " |          Transformed array.\n",
                        " |  \n",
                        " |  partial_fit(self, X, y=None, sample_weight=None)\n",
                        " |      Online computation of mean and std on X for later scaling.\n",
                        " |      \n",
                        " |      All of X is processed as a single batch. This is intended for cases\n",
                        " |      when :meth:`fit` is not feasible due to very large number of\n",
                        " |      `n_samples` or because X is read from a continuous stream.\n",
                        " |      \n",
                        " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
                        " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
                        " |      for computing the sample variance: Analysis and recommendations.\"\n",
                        " |      The American Statistician 37.3 (1983): 242-247:\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
                        " |          The data used to compute the mean and standard deviation\n",
                        " |          used for later scaling along the features axis.\n",
                        " |      \n",
                        " |      y : None\n",
                        " |          Ignored.\n",
                        " |      \n",
                        " |      sample_weight : array-like of shape (n_samples,), default=None\n",
                        " |          Individual weights for each sample.\n",
                        " |      \n",
                        " |          .. versionadded:: 0.24\n",
                        " |             parameter *sample_weight* support to StandardScaler.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      self : object\n",
                        " |          Fitted scaler.\n",
                        " |  \n",
                        " |  set_fit_request(self: sklearn.preprocessing._data.StandardScaler, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler\n",
                        " |      Request metadata passed to the ``fit`` method.\n",
                        " |      \n",
                        " |      Note that this method is only relevant if\n",
                        " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
                        " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
                        " |      mechanism works.\n",
                        " |      \n",
                        " |      The options for each parameter are:\n",
                        " |      \n",
                        " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
                        " |      \n",
                        " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
                        " |      \n",
                        " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
                        " |      \n",
                        " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
                        " |      \n",
                        " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
                        " |      existing request. This allows you to change the request for some\n",
                        " |      parameters and not others.\n",
                        " |      \n",
                        " |      .. versionadded:: 1.3\n",
                        " |      \n",
                        " |      .. note::\n",
                        " |          This method is only relevant if this estimator is used as a\n",
                        " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
                        " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
                        " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      self : object\n",
                        " |          The updated object.\n",
                        " |  \n",
                        " |  set_inverse_transform_request(self: sklearn.preprocessing._data.StandardScaler, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler\n",
                        " |      Request metadata passed to the ``inverse_transform`` method.\n",
                        " |      \n",
                        " |      Note that this method is only relevant if\n",
                        " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
                        " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
                        " |      mechanism works.\n",
                        " |      \n",
                        " |      The options for each parameter are:\n",
                        " |      \n",
                        " |      - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided.\n",
                        " |      \n",
                        " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``.\n",
                        " |      \n",
                        " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
                        " |      \n",
                        " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
                        " |      \n",
                        " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
                        " |      existing request. This allows you to change the request for some\n",
                        " |      parameters and not others.\n",
                        " |      \n",
                        " |      .. versionadded:: 1.3\n",
                        " |      \n",
                        " |      .. note::\n",
                        " |          This method is only relevant if this estimator is used as a\n",
                        " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
                        " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
                        " |          Metadata routing for ``copy`` parameter in ``inverse_transform``.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      self : object\n",
                        " |          The updated object.\n",
                        " |  \n",
                        " |  set_partial_fit_request(self: sklearn.preprocessing._data.StandardScaler, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler\n",
                        " |      Request metadata passed to the ``partial_fit`` method.\n",
                        " |      \n",
                        " |      Note that this method is only relevant if\n",
                        " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
                        " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
                        " |      mechanism works.\n",
                        " |      \n",
                        " |      The options for each parameter are:\n",
                        " |      \n",
                        " |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
                        " |      \n",
                        " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
                        " |      \n",
                        " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
                        " |      \n",
                        " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
                        " |      \n",
                        " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
                        " |      existing request. This allows you to change the request for some\n",
                        " |      parameters and not others.\n",
                        " |      \n",
                        " |      .. versionadded:: 1.3\n",
                        " |      \n",
                        " |      .. note::\n",
                        " |          This method is only relevant if this estimator is used as a\n",
                        " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
                        " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
                        " |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      self : object\n",
                        " |          The updated object.\n",
                        " |  \n",
                        " |  set_transform_request(self: sklearn.preprocessing._data.StandardScaler, *, copy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.preprocessing._data.StandardScaler\n",
                        " |      Request metadata passed to the ``transform`` method.\n",
                        " |      \n",
                        " |      Note that this method is only relevant if\n",
                        " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
                        " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
                        " |      mechanism works.\n",
                        " |      \n",
                        " |      The options for each parameter are:\n",
                        " |      \n",
                        " |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
                        " |      \n",
                        " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
                        " |      \n",
                        " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
                        " |      \n",
                        " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
                        " |      \n",
                        " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
                        " |      existing request. This allows you to change the request for some\n",
                        " |      parameters and not others.\n",
                        " |      \n",
                        " |      .. versionadded:: 1.3\n",
                        " |      \n",
                        " |      .. note::\n",
                        " |          This method is only relevant if this estimator is used as a\n",
                        " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
                        " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      copy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
                        " |          Metadata routing for ``copy`` parameter in ``transform``.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      self : object\n",
                        " |          The updated object.\n",
                        " |  \n",
                        " |  transform(self, X, copy=None)\n",
                        " |      Perform standardization by centering and scaling.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
                        " |          The data used to scale along the features axis.\n",
                        " |      copy : bool, default=None\n",
                        " |          Copy the input X or not.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
                        " |          Transformed array.\n",
                        " |  \n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Data and other attributes defined here:\n",
                        " |  \n",
                        " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
                        " |  \n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Methods inherited from sklearn.base.OneToOneFeatureMixin:\n",
                        " |  \n",
                        " |  get_feature_names_out(self, input_features=None)\n",
                        " |      Get output feature names for transformation.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      input_features : array-like of str or None, default=None\n",
                        " |          Input features.\n",
                        " |      \n",
                        " |          - If `input_features` is `None`, then `feature_names_in_` is\n",
                        " |            used as feature names in. If `feature_names_in_` is not defined,\n",
                        " |            then the following input feature names are generated:\n",
                        " |            `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
                        " |          - If `input_features` is an array-like, then `input_features` must\n",
                        " |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      feature_names_out : ndarray of str objects\n",
                        " |          Same as input features.\n",
                        " |  \n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Data descriptors inherited from sklearn.base.OneToOneFeatureMixin:\n",
                        " |  \n",
                        " |  __dict__\n",
                        " |      dictionary for instance variables (if defined)\n",
                        " |  \n",
                        " |  __weakref__\n",
                        " |      list of weak references to the object (if defined)\n",
                        " |  \n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Methods inherited from sklearn.base.TransformerMixin:\n",
                        " |  \n",
                        " |  fit_transform(self, X, y=None, **fit_params)\n",
                        " |      Fit to data, then transform it.\n",
                        " |      \n",
                        " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
                        " |      and returns a transformed version of `X`.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      X : array-like of shape (n_samples, n_features)\n",
                        " |          Input samples.\n",
                        " |      \n",
                        " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
                        " |          Target values (None for unsupervised transformations).\n",
                        " |      \n",
                        " |      **fit_params : dict\n",
                        " |          Additional fit parameters.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
                        " |          Transformed array.\n",
                        " |  \n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
                        " |  \n",
                        " |  set_output(self, *, transform=None)\n",
                        " |      Set output container.\n",
                        " |      \n",
                        " |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
                        " |      for an example on how to use the API.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      transform : {\"default\", \"pandas\"}, default=None\n",
                        " |          Configure output of `transform` and `fit_transform`.\n",
                        " |      \n",
                        " |          - `\"default\"`: Default output format of a transformer\n",
                        " |          - `\"pandas\"`: DataFrame output\n",
                        " |          - `None`: Transform configuration is unchanged\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      self : estimator instance\n",
                        " |          Estimator instance.\n",
                        " |  \n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
                        " |  \n",
                        " |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs) from builtins.type\n",
                        " |      This method is called when a class is subclassed.\n",
                        " |      \n",
                        " |      The default implementation does nothing. It may be\n",
                        " |      overridden to extend subclasses.\n",
                        " |  \n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Methods inherited from sklearn.base.BaseEstimator:\n",
                        " |  \n",
                        " |  __getstate__(self)\n",
                        " |  \n",
                        " |  __repr__(self, N_CHAR_MAX=700)\n",
                        " |      Return repr(self).\n",
                        " |  \n",
                        " |  __setstate__(self, state)\n",
                        " |  \n",
                        " |  __sklearn_clone__(self)\n",
                        " |  \n",
                        " |  get_params(self, deep=True)\n",
                        " |      Get parameters for this estimator.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      deep : bool, default=True\n",
                        " |          If True, will return the parameters for this estimator and\n",
                        " |          contained subobjects that are estimators.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      params : dict\n",
                        " |          Parameter names mapped to their values.\n",
                        " |  \n",
                        " |  set_params(self, **params)\n",
                        " |      Set the parameters of this estimator.\n",
                        " |      \n",
                        " |      The method works on simple estimators as well as on nested objects\n",
                        " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
                        " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
                        " |      possible to update each component of a nested object.\n",
                        " |      \n",
                        " |      Parameters\n",
                        " |      ----------\n",
                        " |      **params : dict\n",
                        " |          Estimator parameters.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      self : estimator instance\n",
                        " |          Estimator instance.\n",
                        " |  \n",
                        " |  ----------------------------------------------------------------------\n",
                        " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
                        " |  \n",
                        " |  get_metadata_routing(self)\n",
                        " |      Get metadata routing of this object.\n",
                        " |      \n",
                        " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
                        " |      mechanism works.\n",
                        " |      \n",
                        " |      Returns\n",
                        " |      -------\n",
                        " |      routing : MetadataRequest\n",
                        " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
                        " |          routing information.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "help(StandardScaler)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Help on function train_test_split in module sklearn.model_selection._split:\n",
                        "\n",
                        "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
                        "    Split arrays or matrices into random train and test subsets.\n",
                        "    \n",
                        "    Quick utility that wraps input validation,\n",
                        "    ``next(ShuffleSplit().split(X, y))``, and application to input data\n",
                        "    into a single call for splitting (and optionally subsampling) data into a\n",
                        "    one-liner.\n",
                        "    \n",
                        "    Read more in the :ref:`User Guide <cross_validation>`.\n",
                        "    \n",
                        "    Parameters\n",
                        "    ----------\n",
                        "    *arrays : sequence of indexables with same length / shape[0]\n",
                        "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
                        "        matrices or pandas dataframes.\n",
                        "    \n",
                        "    test_size : float or int, default=None\n",
                        "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
                        "        of the dataset to include in the test split. If int, represents the\n",
                        "        absolute number of test samples. If None, the value is set to the\n",
                        "        complement of the train size. If ``train_size`` is also None, it will\n",
                        "        be set to 0.25.\n",
                        "    \n",
                        "    train_size : float or int, default=None\n",
                        "        If float, should be between 0.0 and 1.0 and represent the\n",
                        "        proportion of the dataset to include in the train split. If\n",
                        "        int, represents the absolute number of train samples. If None,\n",
                        "        the value is automatically set to the complement of the test size.\n",
                        "    \n",
                        "    random_state : int, RandomState instance or None, default=None\n",
                        "        Controls the shuffling applied to the data before applying the split.\n",
                        "        Pass an int for reproducible output across multiple function calls.\n",
                        "        See :term:`Glossary <random_state>`.\n",
                        "    \n",
                        "    shuffle : bool, default=True\n",
                        "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
                        "        then stratify must be None.\n",
                        "    \n",
                        "    stratify : array-like, default=None\n",
                        "        If not None, data is split in a stratified fashion, using this as\n",
                        "        the class labels.\n",
                        "        Read more in the :ref:`User Guide <stratification>`.\n",
                        "    \n",
                        "    Returns\n",
                        "    -------\n",
                        "    splitting : list, length=2 * len(arrays)\n",
                        "        List containing train-test split of inputs.\n",
                        "    \n",
                        "        .. versionadded:: 0.16\n",
                        "            If the input is sparse, the output will be a\n",
                        "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
                        "            input type.\n",
                        "    \n",
                        "    Examples\n",
                        "    --------\n",
                        "    >>> import numpy as np\n",
                        "    >>> from sklearn.model_selection import train_test_split\n",
                        "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
                        "    >>> X\n",
                        "    array([[0, 1],\n",
                        "           [2, 3],\n",
                        "           [4, 5],\n",
                        "           [6, 7],\n",
                        "           [8, 9]])\n",
                        "    >>> list(y)\n",
                        "    [0, 1, 2, 3, 4]\n",
                        "    \n",
                        "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
                        "    ...     X, y, test_size=0.33, random_state=42)\n",
                        "    ...\n",
                        "    >>> X_train\n",
                        "    array([[4, 5],\n",
                        "           [0, 1],\n",
                        "           [6, 7]])\n",
                        "    >>> y_train\n",
                        "    [2, 0, 3]\n",
                        "    >>> X_test\n",
                        "    array([[2, 3],\n",
                        "           [8, 9]])\n",
                        "    >>> y_test\n",
                        "    [1, 4]\n",
                        "    \n",
                        "    >>> train_test_split(y, shuffle=False)\n",
                        "    [[0, 1, 2], [3, 4]]\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "help(train_test_split)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
